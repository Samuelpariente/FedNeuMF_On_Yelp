{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_csv(file_path, split_ratio=0.5):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Calculate the split index\n",
    "    split_index = int(len(df) * split_ratio)\n",
    "\n",
    "    # Split the DataFrame into two parts\n",
    "    df_part1 = df[:split_index]\n",
    "    df_part2 = df[split_index:]\n",
    "\n",
    "    # Save the parts as new CSV files\n",
    "    df_part1.to_csv('part1.csv', index=False)\n",
    "    df_part2.to_csv('part2.csv', index=False)\n",
    "    df_part1.to_csv(\"data/yelp-train_1.csv\")\n",
    "    df_part2.to_csv(\"data/yelp-train_2.csv\")\n",
    "# Example usage\n",
    "split_csv('data/yelp-train.csv', split_ratio=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://d2l.ai/chapter_recommender-systems/neumf.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/AmanPriyanshu/Federated-Recommendation-Neural-Collaborative-Filtering/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Ach113/federeco/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  business_id  stars\n",
      "0   827556        88673      5\n",
      "1   827556        36288      5\n",
      "2   827556        30490      4\n",
      "3  1202231        28754      4\n",
      "4  1202231       122144      4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Parquet files (column names are read automatically)\n",
    "train_df = pd.read_parquet(\"data/yelp-train.parquet\")\n",
    "test_df = pd.read_parquet(\"data/yelp-test.parquet\")\n",
    "\n",
    "train_df.columns = ['user_id', 'business_id', 'stars']\n",
    "test_df.columns = ['user_id', 'business_id', 'stars']\n",
    "\n",
    "# Concatenate train and test DataFrames\n",
    "df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_dataset_stats(df):\n",
    "    dataset_stats = {\n",
    "        'Categories': ['#Interactions', '#Users', '#Items'],\n",
    "        'Values': [len(df), df['user_id'].nunique(), df['business_id'].nunique()],\n",
    "        'Colors': ['#7600FF', '#FF7800', '#009D00']\n",
    "    }\n",
    "    return pd.DataFrame(dataset_stats)\n",
    "\n",
    "def calculate_data_quantity_heterogeneity(df):\n",
    "    # Count the number of interactions (reviews/ratings) per business\n",
    "    interactions_per_business = df.groupby('business_id').size()\n",
    "\n",
    "    # Calculate the required statistics\n",
    "    min_interactions = interactions_per_business.min()\n",
    "    max_interactions = interactions_per_business.max()\n",
    "    avg_interactions = interactions_per_business.mean()\n",
    "    std_dev_interactions = interactions_per_business.std()\n",
    "    variance_interactions = std_dev_interactions ** 2\n",
    "\n",
    "    # Round the values\n",
    "    min_interactions = round(min_interactions, 2)  # round to 2 decimal places\n",
    "    max_interactions = round(max_interactions, 2)\n",
    "    avg_interactions = round(avg_interactions, 2)\n",
    "    std_dev_interactions = round(std_dev_interactions, 2)\n",
    "    variance_interactions = round(variance_interactions, 2)\n",
    "\n",
    "    data_quantity_heterogeneity = {\n",
    "        'Metrics': ['#Min', '#Max', 'Average', 'St. Dev.', 'Variance'],\n",
    "        'Values': [min_interactions, max_interactions, avg_interactions, std_dev_interactions, variance_interactions],\n",
    "        'Colors': ['#00A3E8', '#B1001E', '#7600FF', '#FF7800', '#009D00']\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(data_quantity_heterogeneity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           "#7600FF",
           "#FF7800",
           "#009D00"
          ]
         },
         "text": [
          5261666,
          1326101,
          174567
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          "#Interactions",
          "#Users",
          "#Items"
         ],
         "xaxis": "x",
         "y": [
          5261666,
          1326101,
          174567
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": [
           "#00A3E8",
           "#B1001E",
           "#7600FF",
           "#FF7800",
           "#009D00"
          ]
         },
         "text": [
          1,
          7362,
          30.14,
          98.22,
          9646.52
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          "#Min",
          "#Max",
          "Average",
          "St. Dev.",
          "Variance"
         ],
         "xaxis": "x2",
         "y": [
          1,
          7362,
          30.14,
          98.22,
          9646.52
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Evaluation Dataset Statistics",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Data Quantity Heterogeneity",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 600,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 1200,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "type": "log"
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's try again with the correct import for make_subplots\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "# Define the data for the plots\n",
    "dataset_stats = calculate_dataset_stats(df)\n",
    "\n",
    "data_quantity_heterogeneity = calculate_data_quantity_heterogeneity(df)\n",
    "\n",
    "# Create the subplots\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Evaluation Dataset Statistics\", \"Data Quantity Heterogeneity\"))\n",
    "\n",
    "# Add the dataset statistics bar chart\n",
    "fig.add_trace(\n",
    "    go.Bar(x=dataset_stats['Categories'], \n",
    "           y=dataset_stats['Values'], \n",
    "           marker_color=dataset_stats['Colors'],\n",
    "           text=dataset_stats['Values'],\n",
    "           textposition='auto'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add the data quantity heterogeneity bar chart\n",
    "fig.add_trace(\n",
    "    go.Bar(x=data_quantity_heterogeneity['Metrics'], \n",
    "           y=data_quantity_heterogeneity['Values'], \n",
    "           marker_color=data_quantity_heterogeneity['Colors'],\n",
    "           text=data_quantity_heterogeneity['Values'],\n",
    "           textposition='auto'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update y-axis to log scale for the first subplot\n",
    "fig.update_yaxes(type=\"log\", row=1, col=1)\n",
    "\n",
    "# Update layout for better visibility\n",
    "fig.update_layout(showlegend=False, height=600, width=1200)\n",
    "\n",
    "# Show the plots\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronal Colaborative Filtering implementation with Cornac "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the Parquet files (column names are read automatically)\n",
    "train_df = pd.read_parquet(\"data/yelp-train.parquet\").head(50000)\n",
    "test_df = pd.read_parquet(\"data/yelp-test.parquet\").head(50000)\n",
    "\n",
    "train_df.columns = ['user_id', 'business_id', 'stars']\n",
    "test_df.columns = ['user_id', 'business_id', 'stars']\n",
    "\n",
    "# Concatenate train and test DataFrames\n",
    "df = pd.concat([train_df, test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating_threshold = 3.0\n",
      "exclude_unknowns = True\n",
      "---\n",
      "Training data:\n",
      "Number of users = 41288\n",
      "Number of items = 40086\n",
      "Number of ratings = 80000\n",
      "Max rating = 5.0\n",
      "Min rating = 1.0\n",
      "Global mean = 3.7\n",
      "---\n",
      "Test data:\n",
      "Number of users = 41288\n",
      "Number of items = 40086\n",
      "Number of ratings = 7685\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Total users = 41288\n",
      "Total items = 40086\n",
      "\n",
      "[GMF] Training started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc640e23f8c3489f9d573c44239e558a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GMF] Evaluation started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67873879a624d9da4ce29734cfaa007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ranking:   0%|          | 0/3017 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MLP] Training started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f2a899010e422aab99616b1412c251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MLP] Evaluation started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9d5d73a813415e96339bd58d83339e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ranking:   0%|          | 0/3017 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[NeuMF] Training started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163594da934e4fbc82374bff4b2063a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[NeuMF] Evaluation started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58013eebeb846d49c2f8b001c5aeda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ranking:   0%|          | 0/3017 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[NeuMF_pretrained] Training started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2494271aa34bedb6096ba8932e1405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[NeuMF_pretrained] Evaluation started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8c0d1c91be4d5d8811c863730c1232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ranking:   0%|          | 0/3017 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "                 |    MAP | NDCG@10 | Precision@10 | Recall@10 | Train (s) | Test (s)\n",
      "---------------- + ------ + ------- + ------------ + --------- + --------- + --------\n",
      "GMF              | 0.0087 |  0.0097 |       0.0031 |    0.0159 |  213.0201 |  13.3559\n",
      "MLP              | 0.0087 |  0.0095 |       0.0033 |    0.0157 |  220.0058 |  19.1761\n",
      "NeuMF            | 0.0085 |  0.0097 |       0.0036 |    0.0148 |  223.2235 |  19.9141\n",
      "NeuMF_pretrained | 0.0097 |  0.0101 |       0.0032 |    0.0153 |  219.7899 |  19.3742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cornac\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.data import Reader\n",
    "from cornac.metrics import Precision, Recall, NDCG, MAP\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp_file:\n",
    "    df.to_csv(tmp_file.name, index=False, header=False, sep='\\t')\n",
    "\n",
    "# Read the data using Cornac's Reader\n",
    "reader = Reader()\n",
    "feedback = reader.read(tmp_file.name, fmt='UIR')\n",
    "\n",
    "# Define an evaluation method to split feedback into train and test sets\n",
    "ratio_split = RatioSplit(\n",
    "    data=feedback,\n",
    "    test_size=0.2,\n",
    "    rating_threshold=3.0,\n",
    "    seed=123,\n",
    "    exclude_unknowns=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "backend = \"tensorflow\"  # or 'pytorch'\n",
    "\n",
    "# Instantiate the recommender models to be compared\n",
    "gmf = cornac.models.GMF(\n",
    "    num_factors=8,\n",
    "    num_epochs=10,\n",
    "    learner=\"adam\",\n",
    "    backend=backend,\n",
    "    batch_size=256,\n",
    "    lr=0.001,\n",
    "    num_neg=50,\n",
    "    seed=123,\n",
    ")\n",
    "mlp = cornac.models.MLP(\n",
    "    layers=[64, 32, 16, 8],\n",
    "    act_fn=\"tanh\",\n",
    "    learner=\"adam\",\n",
    "    backend=backend,\n",
    "    num_epochs=10,\n",
    "    batch_size=256,\n",
    "    lr=0.001,\n",
    "    num_neg=50,\n",
    "    seed=123,\n",
    ")\n",
    "neumf1 = cornac.models.NeuMF(\n",
    "    num_factors=8,\n",
    "    layers=[64, 32, 16, 8],\n",
    "    act_fn=\"tanh\",\n",
    "    learner=\"adam\",\n",
    "    backend=backend,\n",
    "    num_epochs=10,\n",
    "    batch_size=256,\n",
    "    lr=0.001,\n",
    "    num_neg=50,\n",
    "    seed=123,\n",
    ")\n",
    "neumf2 = cornac.models.NeuMF(\n",
    "    name=\"NeuMF_pretrained\",\n",
    "    learner=\"sgd\",\n",
    "    backend=backend,\n",
    "    num_epochs=10,\n",
    "    batch_size=256,\n",
    "    lr=0.001,\n",
    "    num_neg=50,\n",
    "    seed=123,\n",
    "    num_factors=gmf.num_factors,\n",
    "    layers=mlp.layers,\n",
    "    act_fn=mlp.act_fn,\n",
    ").from_pretrained(gmf, mlp, alpha=0.5)\n",
    "\n",
    "\n",
    "# Specify the metrics to evaluate the model\n",
    "metrics = [\n",
    "    Precision(k=10),\n",
    "    Recall(k=10),\n",
    "    NDCG(k=10),\n",
    "    MAP(),\n",
    "    Precision(k=10),\n",
    "    Recall(k=10),\n",
    "]\n",
    "\n",
    "# Put everything together into an experiment and run it\n",
    "cornac.Experiment(\n",
    "    eval_method=ratio_split,\n",
    "    models=[\n",
    "        gmf,\n",
    "        mlp,\n",
    "        neumf1,\n",
    "        neumf2,\n",
    "    ],\n",
    "    metrics=metrics,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuMF from scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand more the structure of NeuMF let's implement it from scratch with pytorch :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_num_user(df,n):\n",
    "    user_interactions = df.groupby('user_id').size()\n",
    "    users_with_min_interactions = user_interactions[user_interactions >= 5]\n",
    "    selected_users = pd.Series(users_with_min_interactions.index).sample(n=200, random_state=1) \n",
    "    selected_df = df[df['user_id'].isin(selected_users)]\n",
    "    return selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = filter_num_user(df,200)\n",
    "threshold = 3\n",
    "df['feedback'] = (df['stars'] >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_factors, num_users, num_items, layers):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.P = nn.Embedding(num_users, num_factors)\n",
    "        self.Q = nn.Embedding(num_items, num_factors)\n",
    "        self.U = nn.Embedding(num_users, num_factors)\n",
    "        self.V = nn.Embedding(num_items, num_factors)\n",
    "\n",
    "        # MLP layers\n",
    "        mlp_layers = []\n",
    "        input_size = 2 * num_factors\n",
    "        for layer_size in layers:\n",
    "            mlp_layers.append(nn.Linear(input_size, layer_size))\n",
    "            mlp_layers.append(nn.Tanh())\n",
    "            input_size = layer_size\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "\n",
    "        # Prediction layer\n",
    "        self.prediction_layer = nn.Linear(input_size + num_factors, 1)\n",
    "\n",
    "    def forward(self, user_id, item_id):\n",
    "        p_mf = self.P(user_id)\n",
    "        q_mf = self.Q(item_id)\n",
    "        gmf = p_mf * q_mf\n",
    "\n",
    "        p_mlp = self.U(user_id)\n",
    "        q_mlp = self.V(item_id)\n",
    "        mlp = torch.cat((p_mlp, q_mlp), dim=1)\n",
    "        mlp = self.mlp(mlp)\n",
    "\n",
    "        con_res = torch.cat((gmf, mlp), dim=1)\n",
    "        output = self.prediction_layer(con_res)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df['feedback'] = df['stars'].apply(lambda x: 1 if x >= 3 else 0)\n",
    "\n",
    "# Encoding user_id and business_id\n",
    "user_encoder = LabelEncoder()\n",
    "business_encoder = LabelEncoder()\n",
    "\n",
    "df['user_id'] = user_encoder.fit_transform(df['user_id'])\n",
    "df['business_id'] = business_encoder.fit_transform(df['business_id'])\n",
    "\n",
    "# Splitting the dataset\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Number of unique users and businesses\n",
    "num_users = df['user_id'].nunique()\n",
    "num_items = df['business_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_df = train_df[train_df['feedback'] == 1]\n",
    "negative_df = train_df[train_df['feedback'] == 0]\n",
    "\n",
    "# Sample negative instances\n",
    "sampled_negative_df = negative_df.sample(n=len(positive_df) * 50, replace=True, random_state=42)\n",
    "\n",
    "# Combine positive and sampled negative instances\n",
    "combined_df = pd.concat([positive_df, sampled_negative_df]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_df = test_df[test_df['feedback'] == 1]\n",
    "negative_df = test_df[test_df['feedback'] == 0]\n",
    "\n",
    "# Sample negative instances\n",
    "sampled_negative_df_test = negative_df.sample(n=len(positive_df) * 50, replace=True, random_state=42)\n",
    "\n",
    "# Combine positive and sampled negative instances\n",
    "combined_df_test = pd.concat([positive_df, sampled_negative_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ImplicitDataset(Dataset):\n",
    "    def __init__(self, users, items, feedback):\n",
    "        self.users = torch.tensor(users, dtype=torch.long)\n",
    "        self.items = torch.tensor(items, dtype=torch.long)\n",
    "        self.feedback = torch.tensor(feedback, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.feedback[idx]\n",
    "\n",
    "# Create datasets\n",
    "combined_dataset = ImplicitDataset(combined_df['user_id'].values, combined_df['business_id'].values, combined_df['feedback'].values)\n",
    "combined_dataset_test = ImplicitDataset(combined_df_test['user_id'].values, combined_df_test['business_id'].values, combined_df_test['feedback'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|██████████| 445/445 [00:01<00:00, 314.57it/s, loss=0.0438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.20585379786370844, Time per epoch: 1.42 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]: 100%|██████████| 445/445 [00:01<00:00, 361.11it/s, loss=0.28]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: 0.0885255539057295, Time per epoch: 1.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]: 100%|██████████| 445/445 [00:01<00:00, 368.66it/s, loss=0.308] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: 0.06456000215551827, Time per epoch: 1.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]: 100%|██████████| 445/445 [00:01<00:00, 349.72it/s, loss=0.00621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Loss: 0.04183145686984062, Time per epoch: 1.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]: 100%|██████████| 445/445 [00:01<00:00, 368.14it/s, loss=0.00366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Loss: 0.025193036507731408, Time per epoch: 1.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]: 100%|██████████| 445/445 [00:01<00:00, 366.50it/s, loss=0.0014] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Average Loss: 0.013765327506855633, Time per epoch: 1.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]: 100%|██████████| 445/445 [00:01<00:00, 349.93it/s, loss=0.00649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Average Loss: 0.00843722742009029, Time per epoch: 1.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]: 100%|██████████| 445/445 [00:01<00:00, 362.85it/s, loss=0.00026] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Average Loss: 0.005179137611118135, Time per epoch: 1.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]: 100%|██████████| 445/445 [00:01<00:00, 365.62it/s, loss=0.00465] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Average Loss: 0.0033890667591202125, Time per epoch: 1.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]: 100%|██████████| 445/445 [00:01<00:00, 350.54it/s, loss=0.000126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Average Loss: 0.0023529899889325953, Time per epoch: 1.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Instantiate the model\n",
    "model = NeuMF(8, num_users, num_items, layers=[64, 32, 16, 8]).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Criterion\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "epochs = 10\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time() \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_loader = DataLoader(combined_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "    # Using tqdm for progress tracking\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for user_ids, item_ids, feedback in loop:\n",
    "        user_ids, item_ids, feedback = user_ids.to(device), item_ids.to(device), feedback.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(user_ids, item_ids).squeeze()\n",
    "        loss = criterion(predictions, feedback)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update tqdm loop display\n",
    "        loop.set_description(f'Epoch [{epoch + 1}/{epochs}]')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        epoch_duration = time.time() - start_time  \n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {total_loss / len(train_loader)}, Time per epoch: {epoch_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 73/438 [00:00<00:00, 729.69it/s]/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      " 36%|███▌      | 158/438 [00:00<00:00, 797.43it/s]/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      " 55%|█████▌    | 243/438 [00:00<00:00, 819.15it/s]/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      " 75%|███████▍  | 328/438 [00:00<00:00, 831.19it/s]/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      " 94%|█████████▍| 413/438 [00:00<00:00, 834.77it/s]/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/Users/samuelpariente/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "100%|██████████| 438/438 [00:00<00:00, 818.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP': 0.7314821688140779, 'NDCG@10': 61.080636974441134, 'Precision@10': 0.12671234, 'Recall@10': 0.7332572298779335}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def dcg_at_k(scores, k):\n",
    "    \"\"\"Compute DCG at rank k.\"\"\"\n",
    "    scores = scores[:k]  # Consider only top-k scores\n",
    "    return np.sum((np.power(2, scores) - 1) / np.log2(np.arange(2, k + 2)), dtype=np.float32)\n",
    "\n",
    "def ndcg_at_k(true_scores, pred_scores, k):\n",
    "    \"\"\"Compute NDCG at rank k.\"\"\"\n",
    "    order = np.argsort(pred_scores)[::-1]\n",
    "    true_scores_sorted = true_scores[order]\n",
    "\n",
    "    # Compute DCG for the sorted true scores\n",
    "    idcg = dcg_at_k(true_scores_sorted, k)\n",
    "    if not idcg:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute DCG for the predicted scores\n",
    "    dcg = dcg_at_k(pred_scores[order], k)\n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(model, data_loader, k=10):\n",
    "    model.eval()\n",
    "    AP_scores = []\n",
    "    NDCG_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(data_loader, leave=True)\n",
    "        for user_ids, item_ids, actuals in loop:\n",
    "            user_ids, item_ids, actuals = user_ids.to(device), item_ids.to(device), actuals.to(device)\n",
    "            predictions = model(user_ids, item_ids).squeeze()\n",
    "            \n",
    "            # Converting predictions to CPU for metric calculation\n",
    "            predictions = predictions.cpu().numpy()\n",
    "            actuals = actuals.cpu().numpy()\n",
    "            \n",
    "            # Rank items and select top-k\n",
    "            topk_indices = np.argsort(predictions)[::-1][:k]\n",
    "            topk_preds = predictions[topk_indices]\n",
    "            topk_actuals = actuals[topk_indices]\n",
    "\n",
    "            # Calculate metrics\n",
    "            AP_scores.append(average_precision_score(actuals, predictions))\n",
    "            NDCG_scores.append(ndcg_at_k(actuals, predictions, k=k))  # Removed the extra list brackets\n",
    "            precision_scores.append(np.mean(topk_actuals))\n",
    "            if np.sum(actuals) == 0:\n",
    "                recall = 0.0  # or handle it in a way that makes sense for your context\n",
    "            else:\n",
    "                recall = np.sum(topk_actuals) / np.sum(actuals)\n",
    "\n",
    "            recall_scores.append(recall)\n",
    "\n",
    "    return {\n",
    "        \"MAP\": np.mean(AP_scores),\n",
    "        \"NDCG@10\": np.mean(NDCG_scores),\n",
    "        \"Precision@10\": np.mean(precision_scores),\n",
    "        \"Recall@10\": np.mean(recall_scores)\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage after training\n",
    "test_loader = DataLoader(combined_dataset_test, batch_size=256, shuffle=True) \n",
    "metrics = compute_metrics(model, test_loader)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'business_id': 'item_id'}, inplace=True)\n",
    "df.rename(columns={'stars': 'rating'}, inplace=True)\n",
    "df = df.drop(columns=\"feedback\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df\n",
    "train_df.to_csv(\"data/train_exp_200.csv\")\n",
    "test_df.to_csv(\"data/test_exp_200.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedNeuMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "MODEL_PARAMETERS = {\n",
    "    'default_model': {\n",
    "        'mf_dim': 8,\n",
    "        'layers': [64, 32, 16, 8],\n",
    "        'reg_layers': [0, 0, 0, 0],\n",
    "        'reg_mf': 0\n",
    "    },\n",
    "    'FedNCF': {\n",
    "        'mf_dim': 16,\n",
    "        'layers': [64, 32, 16, 8],\n",
    "        'reg_layers': [0, 0, 0, 0],\n",
    "        'reg_mf': 0\n",
    "    },\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_NEGATIVES = 50\n",
    "THRESHOLD = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "def load_csv_parts(dataset):\n",
    "    # Load each part\n",
    "    columns = ['user_id', 'item_id', 'rating']\n",
    "    df_part1 = pd.read_csv(os.path.join('data', dataset + '-train_1.csv'), header=None, names=columns)\n",
    "    df_part2 = pd.read_csv(os.path.join('data', dataset + '-train_2.csv'), header=None, names=columns)\n",
    "\n",
    "    return pd.concat([df_part1, df_part2], ignore_index=True)\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "\n",
    "    def __init__(self, dataset: str):\n",
    "        columns = ['user_id', 'item_id', 'rating']\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        #self.train_df = load_csv_parts(dataset).head(80000)\n",
    "        #self.test_df = pd.read_csv(os.path.join('data', dataset + '-test.csv'), names=columns).head(20000)\n",
    "        self.neg_path = os.path.join('data', dataset + '-neg.csv')\n",
    "        self.num_users, self.num_items = self.get_matrix_dim()\n",
    "        print(f'Loaded `{dataset}` dataset: \\nNumber of users - {self.num_users}, Number of items - {self.num_items}')\n",
    "\n",
    "    def get_matrix_dim(self) -> Tuple[int, int]:\n",
    "        \n",
    "        num_users = max(self.train_df['user_id']) + 1\n",
    "        num_items = max(self.train_df['item_id']) + 1\n",
    "        return num_users, num_items\n",
    "\n",
    "    def load_client_train_data(self) -> List[List]:\n",
    "        \n",
    "        mat = sp.sparse.dok_matrix((self.num_users+1, self.num_items+1), dtype=np.float32)\n",
    "\n",
    "        for user, item, rating in self.train_df.values:\n",
    "            if rating >= THRESHOLD:\n",
    "                mat[user, item] = 1.0\n",
    "\n",
    "        client_datas = [[[], [], []] for _ in range(self.num_users)]\n",
    "\n",
    "        for (usr, item) in mat.keys():\n",
    "            client_datas[usr][0].append(usr)\n",
    "            client_datas[usr][1].append(item)\n",
    "            client_datas[usr][2].append(1)\n",
    "            for t in range(NUM_NEGATIVES):\n",
    "                neg = np.random.randint(self.num_items)\n",
    "                while (usr, neg) in mat.keys():\n",
    "                    neg = np.random.randint(self.num_items)\n",
    "                client_datas[usr][0].append(usr)\n",
    "                client_datas[usr][1].append(neg)\n",
    "                client_datas[usr][2].append(0)\n",
    "\n",
    "        return client_datas\n",
    "\n",
    "    def load_test_file(self) -> List[List[int]]:\n",
    "       \n",
    "        return [[user, item] for user, item, _ in self.test_df.values]\n",
    "\n",
    "    def load_negative_file(self) -> List[List[int]]:\n",
    "        \n",
    "        negative_list = []\n",
    "        with open(self.neg_path, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line is not None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                negatives = []\n",
    "                for x in arr[1:]:\n",
    "                    negatives.append(int(x))\n",
    "                negative_list.append(negatives)\n",
    "                line = f.readline()\n",
    "        return negative_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Any, Tuple\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Tuple, Any, Optional, List\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Client2(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, client_id: int):\n",
    "        self.client_id = client_id\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self,\n",
    "              server_model: torch.nn.Module,\n",
    "              local_epochs: int,\n",
    "              learning_rate: float) -> Tuple[dict[str, Any], Tensor]:\n",
    "        \n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_recommendation(self, server_model: torch.nn.Module,\n",
    "                                num_items: int,  k: Optional[int] = 5) -> List[int]:\n",
    "        \n",
    "        pass\n",
    "\n",
    "\n",
    "class Client(Client2):\n",
    "\n",
    "    def __init__(self, client_id: int):\n",
    "        super().__init__(client_id)\n",
    "        self.client_id = client_id\n",
    "        self.client_data = None\n",
    "\n",
    "    def set_client_data(self, data_array: List[np.ndarray]):\n",
    "        self.client_data = pd.DataFrame({\n",
    "            'user_id': data_array[0],\n",
    "            'item_id': data_array[1],\n",
    "            'label': data_array[2]\n",
    "        })\n",
    "\n",
    "    def train(self, server_model: torch.nn.Module, local_epochs: int, learning_rate: float) -> Tuple[dict[str, Any], Tensor]:\n",
    "     \n",
    "        user_input, item_input = self.client_data['user_id'], self.client_data['item_id']\n",
    "        labels = self.client_data['label']\n",
    "\n",
    "        user_input = torch.tensor(user_input, dtype=torch.int, device=DEVICE)\n",
    "        item_input = torch.tensor(item_input, dtype=torch.int, device=DEVICE)\n",
    "        labels = torch.tensor(labels, dtype=torch.int, device=DEVICE)\n",
    "\n",
    "        # utilize dataloader to train in batches\n",
    "        dataset = torch.utils.data.TensorDataset(user_input, item_input, labels)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        # perturb the learning rate\n",
    "        # learning_rate = abs(learning_rate + np.random.normal(0, .01, 1).squeeze())\n",
    "        optimizer = torch.optim.AdamW(server_model.parameters(), lr=learning_rate)\n",
    "        loss = None\n",
    "        for _ in range(local_epochs):\n",
    "            for _, (u, i, l) in enumerate(dataloader):\n",
    "                logits, loss = server_model(u, i, l)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(server_model.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "        return server_model.state_dict(), loss\n",
    "\n",
    "    def generate_recommendation(self, server_model: torch.nn.Module,\n",
    "                                num_items: int,  k: Optional[int] = 5) -> List[int]:\n",
    "        \"\"\"\n",
    "        :param server_model: server model which will be used to generate predictions\n",
    "        :param num_items: total number of unique items in dataset\n",
    "        :param k: number of recommendations to generate\n",
    "        :return: list of `k` movie recommendations\n",
    "        \"\"\"\n",
    "        # get movies that user has not yet interacted with\n",
    "        buisness = set(range(num_items)).difference(set(self.client_data['item_id'].tolist()))\n",
    "        buisness = torch.tensor(list(buisness), dtype=torch.int, device=DEVICE)\n",
    "        client_id = torch.tensor([self.client_id for _ in range(len(buisness))], dtype=torch.int, device=DEVICE)\n",
    "        # obtain predictions in terms of logit per movie\n",
    "        with torch.no_grad():\n",
    "            logits, _ = server_model(client_id, buisness)\n",
    "\n",
    "        rec_dict = {movie: p for movie, p in zip(buisness.tolist(), logits.squeeze().tolist())}\n",
    "        # select top k recommendations\n",
    "        top_k = sorted(rec_dict.items(), key=lambda x: -x[1])[:k]\n",
    "        rec, _ = zip(*top_k)\n",
    "\n",
    "        return rec\n",
    "    \n",
    "\n",
    "    \n",
    "def initialize_clients(dataset: Dataset) -> List[Client2]:\n",
    "    \n",
    "    clients = list()\n",
    "    client_dataset = dataset.load_client_train_data()\n",
    "\n",
    "    for client_id in range(dataset.num_users):\n",
    "        if not client_dataset[client_id][0]:\n",
    "            continue\n",
    "        c = Client(client_id)\n",
    "        c.set_client_data(client_dataset[client_id])\n",
    "        clients.append(c)\n",
    "    return clients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import collections\n",
    "import random\n",
    "import torch\n",
    "import copy\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def plot_loss(epochs: int, loss: List[float], num_clients: int):\n",
    "    \n",
    "    plt.plot(list(range(epochs)), loss)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('BCE Loss')\n",
    "    plt.title(f'Mean loss across {num_clients} clients per epoch')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def sample_clients(clients: List[Client2], num_clients: int) -> Tuple[List[Client2], List[Client2]]:\n",
    "    \n",
    "    sample = clients[:num_clients]\n",
    "    # rotate the list by `num_clients`\n",
    "    clients = clients[num_clients:] + sample\n",
    "\n",
    "    return sample, clients\n",
    "\n",
    "\n",
    "def training_process(server_model: torch.nn.Module,\n",
    "                     all_clients: List[Client2],\n",
    "                     num_clients: int,\n",
    "                     epochs: int,\n",
    "                     local_epochs: int,\n",
    "                     learning_rate: float,\n",
    "                     plot: Optional[bool] = True) -> dict[str, Any]:\n",
    "    \n",
    "    random.shuffle(all_clients)\n",
    "    total_loss = list()\n",
    "    pbar = tqdm.tqdm(range(epochs))\n",
    "    for epoch in pbar:\n",
    "        # sample `num_clients` clients for training\n",
    "        clients, all_clients = sample_clients(all_clients, num_clients)\n",
    "        # apply single round of training\n",
    "        w, loss = single_train_round(server_model, clients, local_epochs, learning_rate)\n",
    "        total_loss.append(loss)\n",
    "        # aggregate weights\n",
    "\n",
    "        updated_server_weights = federated_averaging(w)\n",
    "        # set aggregated weights to server model\n",
    "        server_model.load_state_dict(updated_server_weights)\n",
    "        # display progress bar with epochs & mean loss of single training round\n",
    "        pbar.set_description(f'epoch: {epoch+1}, loss: {loss:.2f}')\n",
    "\n",
    "    if plot:\n",
    "        plot_loss(epochs, total_loss, num_clients)\n",
    "\n",
    "    return server_model.state_dict()\n",
    "\n",
    "\n",
    "def single_train_round(server_model: torch.nn.Module,\n",
    "                       clients: List[Client],\n",
    "                       local_epochs: int,\n",
    "                       learning_rate: float) -> Tuple[List[collections.OrderedDict], float]:\n",
    "    \n",
    "    client_weights = list()\n",
    "    mean_loss = 0\n",
    "    for client in clients:\n",
    "        server_model_copy = copy.deepcopy(server_model)\n",
    "        weights, loss = client.train(server_model_copy, local_epochs, learning_rate)\n",
    "        mean_loss += float(loss.cpu().detach().numpy())\n",
    "        client_weights.append(weights)\n",
    "    return client_weights, mean_loss / len(client_weights)\n",
    "\n",
    "\n",
    "def federated_averaging(client_weights: List[collections.OrderedDict]) -> collections.OrderedDict:\n",
    "    \n",
    "    keys = client_weights[0].keys()\n",
    "    averages = copy.deepcopy(client_weights[0])\n",
    "\n",
    "    for w in client_weights[1:]:\n",
    "        for key in keys:\n",
    "            averages[key] += w[key]\n",
    "\n",
    "    for key in keys:\n",
    "        averages[key] /= len(client_weights)\n",
    "    return averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from typing import Optional, Tuple\n",
    "from torch import Tensor\n",
    "import torch\n",
    "\n",
    "\n",
    "class NeuralCollaborativeFiltering(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_users: int, num_items: int):\n",
    "        super().__init__()\n",
    "        params = MODEL_PARAMETERS['FedNCF']\n",
    "        layers = params['layers']\n",
    "        mf_dim = params['mf_dim']\n",
    "        mlp_dim = int(layers[0] / 2)\n",
    "\n",
    "        self.mf_embedding_user = torch.nn.Embedding(num_embeddings=num_users, embedding_dim=mf_dim, device=DEVICE)\n",
    "        self.mf_embedding_item = torch.nn.Embedding(num_embeddings=num_items, embedding_dim=mf_dim, device=DEVICE)\n",
    "\n",
    "        self.mlp_embedding_user = torch.nn.Embedding(num_embeddings=num_users, embedding_dim=mlp_dim, device=DEVICE)\n",
    "        self.mlp_embedding_item = torch.nn.Embedding(num_embeddings=num_items, embedding_dim=mlp_dim, device=DEVICE)\n",
    "\n",
    "        self.mlp = torch.nn.ModuleList()\n",
    "        current_dim = 64\n",
    "        for idx in range(1, len(layers)):\n",
    "            self.mlp.append(torch.nn.Linear(current_dim, layers[idx]))\n",
    "            current_dim = layers[idx]\n",
    "            self.mlp.append(torch.nn.ReLU())\n",
    "        self.output_layer = torch.nn.Linear(in_features=24, out_features=1, device=DEVICE)\n",
    "\n",
    "    def forward(self, user_input: Tensor,\n",
    "                item_input: Tensor,\n",
    "                target: Optional[Tensor] = None) -> Tuple[Tensor, Optional[float]]:\n",
    "        # matrix factorization\n",
    "        mf_user_latent = torch.nn.Flatten()(self.mf_embedding_user(user_input))\n",
    "        mf_item_latent = torch.nn.Flatten()(self.mf_embedding_item(item_input))\n",
    "        mf_vector = torch.mul(mf_user_latent, mf_item_latent)\n",
    "        # mlp\n",
    "        mlp_user_latent = torch.nn.Flatten()(self.mlp_embedding_user(user_input))\n",
    "        mlp_item_latent = torch.nn.Flatten()(self.mlp_embedding_item(item_input))\n",
    "        mlp_vector = torch.cat([mlp_user_latent, mlp_item_latent], dim=1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            mlp_vector = layer(mlp_vector)\n",
    "\n",
    "        predict_vector = torch.cat([mf_vector, mlp_vector], dim=1)\n",
    "        logits = self.output_layer(predict_vector)\n",
    "\n",
    "        loss = None\n",
    "        if target is not None:\n",
    "            target = target.view(target.shape[0], 1).to(torch.float32)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, target)\n",
    "\n",
    "        logits = torch.nn.Sigmoid()(logits)\n",
    "\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch.nn\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "import heapq\n",
    "import torch\n",
    "import math\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def dcg_at_k(scores, k):\n",
    "    \"\"\"Compute DCG at rank k.\"\"\"\n",
    "    scores = scores[:k]  # Consider only top-k scores\n",
    "    return np.sum((np.power(2, scores) - 1) / np.log2(np.arange(2, k + 2)), dtype=np.float32)\n",
    "\n",
    "def ndcg_at_k(true_scores, pred_scores, k):\n",
    "    \"\"\"Compute NDCG at rank k.\"\"\"\n",
    "    order = np.argsort(pred_scores)[::-1]\n",
    "    \n",
    "    # Ensure true_scores and pred_scores are numpy arrays\n",
    "    true_scores = np.array(true_scores)\n",
    "    pred_scores = np.array(pred_scores)\n",
    "\n",
    "    true_scores_sorted = true_scores[order[:k]]\n",
    "\n",
    "    # Compute DCG for the sorted true scores\n",
    "    idcg = dcg_at_k(true_scores_sorted, k)\n",
    "    if not idcg:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute DCG for the predicted scores\n",
    "    dcg = dcg_at_k(pred_scores[order[:k]], k)\n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(model: torch.nn.Module, dataset: Dataset, k: int):\n",
    "    model.eval()\n",
    "    AP_scores = []\n",
    "    NDCG_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "\n",
    "    # Load test data and negative samples\n",
    "    test_data = dataset.load_test_file()\n",
    "    negative_samples = dataset.load_negative_file()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_instance, negatives in zip(test_data, negative_samples):\n",
    "            try:\n",
    "                user_id = test_instance[0]\n",
    "                item_id = test_instance[1]\n",
    "                \n",
    "                # Combine positive and negative items for this user\n",
    "                all_items = [item_id] + negatives\n",
    "\n",
    "                # Create tensors for all items and the user\n",
    "                item_input_dev = torch.tensor(all_items, dtype=torch.int, device=DEVICE)\n",
    "                user_input = torch.tensor(np.full(len(item_input_dev), user_id, dtype='int32'), dtype=torch.int, device=DEVICE)\n",
    "\n",
    "                pred, _ = model(user_input, item_input_dev)\n",
    "                predictions = pred.squeeze()\n",
    "\n",
    "                # Check if predictions is a 1-dimensional array\n",
    "                if predictions.ndim == 0:\n",
    "                    predictions = np.array([predictions.cpu().numpy()])\n",
    "                else:\n",
    "                    predictions = predictions.cpu().numpy()\n",
    "\n",
    "                # Actual labels: 1 for the test item, 0 for negatives\n",
    "                actuals = [1] + [0] * len(negatives)\n",
    "\n",
    "                # Rank items and select top-k\n",
    "                topk_indices = np.argsort(predictions)[::-1][:k]\n",
    "                topk_preds = predictions[topk_indices]\n",
    "                topk_actuals = [actuals[i] for i in topk_indices]\n",
    "\n",
    "                # Calculate metrics\n",
    "                AP_scores.append(average_precision_score(actuals, predictions))\n",
    "                NDCG_scores.append(ndcg_at_k(actuals, predictions, k=k))\n",
    "                precision_scores.append(np.mean(topk_actuals))\n",
    "                recall = np.sum(topk_actuals) / np.sum(actuals) if np.sum(actuals) > 0 else 0.0\n",
    "                recall_scores.append(recall)\n",
    "            \n",
    "        \n",
    "            except IndexError as e:\n",
    "                print(\"User not in train\")\n",
    "                # Handle the error or pass\n",
    "                pass\n",
    "\n",
    "    return {\n",
    "        \"MAP\": np.mean(AP_scores),\n",
    "        \"NDCG@10\": np.mean(NDCG_scores),\n",
    "        \"Precision@10\": np.mean(precision_scores),\n",
    "        \"Recall@10\": np.mean(recall_scores)\n",
    "    }\n",
    "\n",
    "\n",
    "def run_server(dataset: Dataset,\n",
    "               num_clients: int,\n",
    "               epochs: int,\n",
    "               local_epochs: int,\n",
    "               learning_rate: float) -> torch.nn.Module:\n",
    "    \n",
    "    # define server side model\n",
    "    server_model = NeuralCollaborativeFiltering(dataset.num_users, dataset.num_items)\n",
    "    server_model.to(DEVICE)\n",
    "\n",
    "    clients = initialize_clients(dataset)\n",
    "    \n",
    "    trained_weights = training_process(server_model, clients, num_clients, epochs, local_epochs, learning_rate)\n",
    "\n",
    "    server_model.load_state_dict(trained_weights)\n",
    "\n",
    "    \n",
    "\n",
    "    # Call the updated compute_metrics function\n",
    "    metrics = compute_metrics(server_model, dataset, 10)\n",
    "    print(metrics)\n",
    "    return server_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded `yelp` dataset: \n",
      "Number of users - 200, Number of items - 3082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 42, loss: 0.08:  42%|████▏     | 42/100 [09:00<12:26, 12.87s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 15\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# run the server to load the existing model or train & save a new one\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     model \u001b[38;5;241m=\u001b[39m run_server(\n\u001b[1;32m      7\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m      8\u001b[0m         num_clients\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[62], line 6\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myelp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# run the server to load the existing model or train & save a new one\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mrun_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 112\u001b[0m, in \u001b[0;36mrun_server\u001b[0;34m(dataset, num_clients, epochs, local_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    108\u001b[0m server_model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    110\u001b[0m clients \u001b[38;5;241m=\u001b[39m initialize_clients(dataset)\n\u001b[0;32m--> 112\u001b[0m trained_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m server_model\u001b[38;5;241m.\u001b[39mload_state_dict(trained_weights)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Call the updated compute_metrics function\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[60], line 44\u001b[0m, in \u001b[0;36mtraining_process\u001b[0;34m(server_model, all_clients, num_clients, epochs, local_epochs, learning_rate, plot)\u001b[0m\n\u001b[1;32m     42\u001b[0m clients, all_clients \u001b[38;5;241m=\u001b[39m sample_clients(all_clients, num_clients)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# apply single round of training\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m w, loss \u001b[38;5;241m=\u001b[39m \u001b[43msingle_train_round\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# aggregate weights\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[60], line 69\u001b[0m, in \u001b[0;36msingle_train_round\u001b[0;34m(server_model, clients, local_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client \u001b[38;5;129;01min\u001b[39;00m clients:\n\u001b[1;32m     68\u001b[0m     server_model_copy \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(server_model)\n\u001b[0;32m---> 69\u001b[0m     weights, loss \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_model_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     mean_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     71\u001b[0m     client_weights\u001b[38;5;241m.\u001b[39mappend(weights)\n",
      "Cell \u001b[0;32mIn[48], line 68\u001b[0m, in \u001b[0;36mClient.train\u001b[0;34m(self, server_model, local_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     65\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     67\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(server_model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m server_model\u001b[38;5;241m.\u001b[39mstate_dict(), loss\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/adamw.py:184\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    174\u001b[0m         group,\n\u001b[1;32m    175\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m         state_steps,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/adamw.py:335\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 335\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/adamw.py:406\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    403\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# update step\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[1;32m    409\u001b[0m param\u001b[38;5;241m.\u001b[39mmul_(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m weight_decay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # instantiate the dataset based on passed argument\n",
    "    dataset = Dataset(\"yelp\")\n",
    "    # run the server to load the existing model or train & save a new one\n",
    "    model = run_server(\n",
    "        dataset=dataset,\n",
    "        num_clients=200,\n",
    "        epochs=100,\n",
    "        local_epochs=3,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fask API Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "\n",
    "def get_updated_weights_from_server(url, model):\n",
    "    \"\"\"\n",
    "    Fetches updated model weights from the server and loads them into the provided model.\n",
    "\n",
    "    :param url: URL of the server endpoint to get updated weights.\n",
    "    :param model: The PyTorch model to update.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # Assuming the server sends a file, we save it temporarily\n",
    "            with open(\"temp_model_weights.pth\", \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "            # Load the downloaded weights into the model\n",
    "            model.load_state_dict(torch.load(\"temp_model_weights.pth\"))\n",
    "            print(\"Model weights updated successfully.\")\n",
    "\n",
    "            # Optionally, delete the temporary file after updating the model\n",
    "            import os\n",
    "            os.remove(\"temp_model_weights.pth\")\n",
    "        else:\n",
    "            print(f\"Failed to download weights. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_weights_to_server(url, model_weights):\n",
    "    \"\"\"\n",
    "    Send model weights to the server.\n",
    "\n",
    "    :param url: URL of the server endpoint expecting the model weights.\n",
    "    :param model_weights: Weights of the model to be sent.\n",
    "    \"\"\"\n",
    "    # Serialize the model weights. You might use pickle or another method depending on your requirements.\n",
    "    serialized_weights = pickle.dumps(model_weights)\n",
    "\n",
    "    # Send the POST request with the serialized weights\n",
    "    response = requests.post(url, files={'file': ('weights.pkl', serialized_weights, 'application/octet-stream')})\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        print(\"Weights successfully sent to the server.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Failed to send weights. Status code: {response.status_code}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import collections\n",
    "import random\n",
    "import torch\n",
    "import copy\n",
    "import tqdm\n",
    "\n",
    "def training_process(server_model: torch.nn.Module,\n",
    "                     all_clients: List[Client2],\n",
    "                     num_clients: int,\n",
    "                     epochs: int,\n",
    "                     local_epochs: int,\n",
    "                     learning_rate: float,\n",
    "                     plot: Optional[bool] = True) -> dict[str, Any]:\n",
    "    \n",
    "\n",
    "    random.shuffle(all_clients)\n",
    "    total_loss = list()\n",
    "    pbar = tqdm.tqdm(range(epochs))\n",
    "    for epoch in pbar:\n",
    "        # sample `num_clients` clients for training\n",
    "        clients, all_clients = sample_clients(all_clients, num_clients)\n",
    "        # apply single round of training\n",
    "        loss = single_train_round(server_model, clients, local_epochs, learning_rate)\n",
    "        total_loss.append(loss)\n",
    "        get_updated_weights_from_server(\"http://127.0.0.1:8080/get_weights\",server_model)\n",
    "        \n",
    "        # display progress bar with epochs & mean loss of single training round\n",
    "        pbar.set_description(f'epoch: {epoch+1}, loss: {loss:.2f}')\n",
    "\n",
    "    if plot:\n",
    "        plot_loss(epochs, total_loss, num_clients)\n",
    "\n",
    "    with open('loss.pkl', 'wb') as f:\n",
    "        pickle.dump(total_loss, f)\n",
    "\n",
    "    return server_model.state_dict()\n",
    "\n",
    "\n",
    "def single_train_round(server_model: torch.nn.Module,\n",
    "                       clients: List[Client],\n",
    "                       local_epochs: int,\n",
    "                       learning_rate: float) -> Tuple[List[collections.OrderedDict], float]:\n",
    "    \n",
    "    mean_loss = 0\n",
    "    for client in clients:\n",
    "        server_model_copy = copy.deepcopy(server_model)\n",
    "        # Calculate the size of the model weights before training\n",
    "        #model_size_before = sum(param.numel() * param.element_size() for param in server_model_copy.parameters())\n",
    "        #model_size_before_mb = model_size_before / (1024 ** 2)\n",
    "\n",
    "        weights, loss = client.train(server_model_copy, local_epochs, learning_rate)\n",
    "\n",
    "        # Calculate the size of the model weights after training\n",
    "        #model_size_after = sum(param.numel() * param.element_size() for param in server_model_copy.parameters())\n",
    "        #model_size_after_mb = model_size_after / (1024 ** 2)\n",
    "\n",
    "        #print(f\"Size of model weights before training: {model_size_before_mb:.2f} MB\")\n",
    "        #print(f\"Size of model weights after training: {model_size_after_mb:.2f} MB\")\n",
    "        # Calculate the size of the model in bytes\n",
    "        model_size_bytes = sum(param.numel() * param.element_size() for param in server_model.parameters())\n",
    "\n",
    "        # Convert the size to megabytes\n",
    "        model_size_mb = model_size_bytes / (1024 ** 2)\n",
    "\n",
    "        print(f\"Model size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "        weights, loss = client.train(server_model_copy, local_epochs, learning_rate)\n",
    "        mean_loss += float(loss.cpu().detach().numpy())\n",
    "        send_weights_to_server(\"http://127.0.0.1:8080/train\",weights)\n",
    "    return  mean_loss / len(clients)\n",
    "\n",
    "\n",
    "\n",
    "def federated_averaging(client_weights: List[collections.OrderedDict]) -> collections.OrderedDict:\n",
    "    \"\"\"\n",
    "    calculates the average of client weights\n",
    "    \"\"\"\n",
    "    keys = client_weights[0].keys()\n",
    "    averages = copy.deepcopy(client_weights[0])\n",
    "\n",
    "    for w in client_weights[1:]:\n",
    "        for key in keys:\n",
    "            averages[key] += w[key]\n",
    "\n",
    "    for key in keys:\n",
    "        averages[key] /= len(client_weights)\n",
    "    return averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded `yelp` dataset: \n",
      "Number of users - 200, Number of items - 3082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 0.61 MB\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /train (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x2d769d940>: Failed to establish a new connection: [Errno 61] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py:496\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connection.py:395\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:1252\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1252\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:1012\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1012\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1015\u001b[0m \n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:952\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 952\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connection.py:243\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connection.py:218\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x2d769d940>: Failed to establish a new connection: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    842\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 844\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /train (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x2d769d940>: Failed to establish a new connection: [Errno 61] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 15\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# run the server to load the existing model or train & save a new one\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     run_server(\n\u001b[1;32m      8\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m      9\u001b[0m         num_clients\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[0;32m---> 15\u001b[0m \u001b[43mAPI_SIMULATION\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[74], line 7\u001b[0m, in \u001b[0;36mAPI_SIMULATION\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myelp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# run the server to load the existing model or train & save a new one\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mrun_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 112\u001b[0m, in \u001b[0;36mrun_server\u001b[0;34m(dataset, num_clients, epochs, local_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    108\u001b[0m server_model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    110\u001b[0m clients \u001b[38;5;241m=\u001b[39m initialize_clients(dataset)\n\u001b[0;32m--> 112\u001b[0m trained_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m server_model\u001b[38;5;241m.\u001b[39mload_state_dict(trained_weights)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Call the updated compute_metrics function\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[73], line 26\u001b[0m, in \u001b[0;36mtraining_process\u001b[0;34m(server_model, all_clients, num_clients, epochs, local_epochs, learning_rate, plot)\u001b[0m\n\u001b[1;32m     24\u001b[0m clients, all_clients \u001b[38;5;241m=\u001b[39m sample_clients(all_clients, num_clients)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# apply single round of training\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43msingle_train_round\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     28\u001b[0m get_updated_weights_from_server(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:8080/get_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m,server_model)\n",
      "Cell \u001b[0;32mIn[73], line 72\u001b[0m, in \u001b[0;36msingle_train_round\u001b[0;34m(server_model, clients, local_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     70\u001b[0m     weights, loss \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mtrain(server_model_copy, local_epochs, learning_rate)\n\u001b[1;32m     71\u001b[0m     mean_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m---> 72\u001b[0m     \u001b[43msend_weights_to_server\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://127.0.0.1:8080/train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m  mean_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(clients)\n",
      "Cell \u001b[0;32mIn[64], line 12\u001b[0m, in \u001b[0;36msend_weights_to_server\u001b[0;34m(url, model_weights)\u001b[0m\n\u001b[1;32m      9\u001b[0m serialized_weights \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(model_weights)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Send the POST request with the serialized weights\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweights.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserialized_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapplication/octet-stream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Check if the request was successful\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /train (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x2d769d940>: Failed to establish a new connection: [Errno 61] Connection refused'))"
     ]
    }
   ],
   "source": [
    "def API_SIMULATION():\n",
    "\n",
    "    # instantiate the dataset based on passed argument\n",
    "    dataset = Dataset(\"yelp\")\n",
    "  \n",
    "    # run the server to load the existing model or train & save a new one\n",
    "    run_server(\n",
    "        dataset=dataset,\n",
    "        num_clients=50,\n",
    "        epochs=10,\n",
    "        local_epochs=3,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "\n",
    "API_SIMULATION()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
